#!/usr/bin/env python3
"""
RudraDB-Opin Best Practices Guide
Comprehensive examples of optimal usage patterns
"""

import numpy as np
import rudradb
import time
from typing import List, Dict, Any, Optional
from datetime import datetime

class BestPracticesDemo:
    """Demonstrate best practices for RudraDB-Opin usage"""
    
    def __init__(self):
        print("üí° RudraDB-Opin Best Practices Demo")
        print("=" * 40)
        print("Learn optimal patterns for relationship-aware vector databases")
    
    def demonstrate_metadata_design(self):
        """Best practices for metadata design"""
        
        print("\nüìã Best Practice #1: Effective Metadata Design")
        print("-" * 45)
        
        db = rudradb.RudraDB()
        
        # ‚úÖ GOOD: Consistent, structured metadata
        def create_good_metadata(title, category, tags, **kwargs):
            """Template for consistent metadata structure"""
            return {
                # Core identification
                "title": title,
                "category": category,
                "subcategory": kwargs.get("subcategory"),
                
                # Classification
                "tags": tags or [],
                "difficulty": kwargs.get("difficulty", "intermediate"),
                "type": kwargs.get("type", "concept"),
                
                # Content info
                "summary": kwargs.get("summary", title[:100]),
                "word_count": kwargs.get("word_count", 0),
                "language": kwargs.get("language", "en"),
                
                # Tracking
                "created_at": datetime.now().isoformat(),
                "version": kwargs.get("version", "1.0"),
                "author": kwargs.get("author", "system"),
                
                # Custom fields
                **{k: v for k, v in kwargs.items() 
                   if k not in ["subcategory", "difficulty", "type", "summary", "word_count", "language", "version", "author"]}
            }
        
        # Example of good metadata usage
        good_examples = [
            {
                "id": "ml_supervised_learning",
                "text": "Supervised learning algorithms learn from labeled training data",
                "metadata": create_good_metadata(
                    title="Supervised Learning Introduction",
                    category="machine_learning",
                    subcategory="algorithms",
                    tags=["supervised", "algorithms", "training"],
                    difficulty="beginner",
                    type="concept",
                    summary="Introduction to supervised learning concepts",
                    word_count=45,
                    estimated_reading_time=2,
                    prerequisites=["ml_basics"]
                )
            },
            {
                "id": "ml_linear_regression",
                "text": "Linear regression finds the best line through data points",
                "metadata": create_good_metadata(
                    title="Linear Regression Explained",
                    category="machine_learning", 
                    subcategory="algorithms",
                    tags=["linear_regression", "supervised", "prediction"],
                    difficulty="intermediate",
                    type="lesson",
                    summary="Learn linear regression for prediction tasks",
                    word_count=38,
                    estimated_reading_time=3,
                    prerequisites=["ml_supervised_learning"]
                )
            }
        ]
        
        print("‚úÖ Good Metadata Example:")
        for example in good_examples:
            embedding = np.random.rand(384).astype(np.float32)
            db.add_vector(example["id"], embedding, example["metadata"])
            
            print(f"   üìÑ {example['id']}:")
            print(f"      Title: {example['metadata']['title']}")
            print(f"      Category: {example['metadata']['category']}/{example['metadata']['subcategory']}")
            print(f"      Tags: {example['metadata']['tags']}")
            print(f"      Difficulty: {example['metadata']['difficulty']}")
            print(f"      Prerequisites: {example['metadata'].get('prerequisites', [])}")
        
        # ‚ùå BAD: Inconsistent, unstructured metadata  
        print("\n‚ùå What NOT to do - Bad Metadata:")
        bad_examples = [
            ("bad_doc_1", {"text": "Some content", "random_field": 123}),
            ("bad_doc_2", {"content": "Different field name", "difficulty": "hard"}),  # Inconsistent
            ("bad_doc_3", {"title": "No other metadata"}),  # Too sparse
        ]
        
        for doc_id, bad_metadata in bad_examples:
            print(f"   ‚ùå {doc_id}: {bad_metadata}")
            print("      Issues: Inconsistent fields, sparse data, no structure")
        
        print("\nüí° Metadata Best Practices:")
        practices = [
            "Use consistent field names across all vectors",
            "Include both identification and classification metadata",
            "Add searchable tags for relationship building",
            "Include difficulty/complexity levels for learning",
            "Store content summaries for quick reference",
            "Add timestamps and version info for tracking",
            "Keep metadata rich but not excessively large",
            "Use standardized values (e.g., difficulty levels)"
        ]
        
        for i, practice in enumerate(practices, 1):
            print(f"   {i}. {practice}")
        
        return db
    
    def demonstrate_relationship_strategies(self, db):
        """Best practices for relationship building"""
        
        print("\nüîó Best Practice #2: Strategic Relationship Building")
        print("-" * 50)
        
        # ‚úÖ GOOD: Strategic relationship building
        def build_smart_relationships(db, doc_id, metadata, max_connections=5):
            """Build strategic, high-quality relationships"""
            
            relationships_added = 0
            doc_category = metadata.get("category")
            doc_subcategory = metadata.get("subcategory")
            doc_tags = set(metadata.get("tags", []))
            doc_difficulty = metadata.get("difficulty")
            doc_type = metadata.get("type")
            prerequisites = metadata.get("prerequisites", [])
            
            # Strategy 1: Hierarchical relationships (most important)
            if prerequisites:
                for prereq_id in prerequisites:
                    if db.vector_exists(prereq_id) and relationships_added < max_connections:
                        db.add_relationship(prereq_id, doc_id, "hierarchical", 0.9,
                                          {"reason": "prerequisite", "auto_detected": True})
                        relationships_added += 1
                        print(f"      ‚úÖ Prerequisite: {prereq_id} ‚Üí {doc_id}")
            
            # Strategy 2: Semantic relationships (same category)
            if doc_category and relationships_added < max_connections:
                similar_docs = [
                    vid for vid in db.list_vectors()
                    if vid != doc_id and db.get_vector(vid)["metadata"].get("category") == doc_category
                ]
                
                # Connect to most similar subcategory
                for other_id in similar_docs[:2]:  # Limit connections
                    if relationships_added >= max_connections:
                        break
                    other_meta = db.get_vector(other_id)["metadata"]
                    if other_meta.get("subcategory") == doc_subcategory:
                        strength = 0.8 if doc_type == other_meta.get("type") else 0.6
                        db.add_relationship(doc_id, other_id, "semantic", strength,
                                          {"reason": "same_subcategory", "auto_detected": True})
                        relationships_added += 1
                        print(f"      ‚úÖ Semantic: {doc_id} ‚Üî {other_id}")
            
            # Strategy 3: Temporal relationships (learning progression)
            if doc_difficulty and relationships_added < max_connections:
                difficulty_levels = {"beginner": 1, "intermediate": 2, "advanced": 3}
                current_level = difficulty_levels.get(doc_difficulty, 2)
                
                for other_id in db.list_vectors():
                    if other_id == doc_id or relationships_added >= max_connections:
                        continue
                    
                    other_meta = db.get_vector(other_id)["metadata"]
                    other_difficulty = other_meta.get("difficulty")
                    other_level = difficulty_levels.get(other_difficulty, 2)
                    
                    # Connect sequential difficulty levels in same category
                    if (other_level == current_level - 1 and 
                        other_meta.get("category") == doc_category):
                        db.add_relationship(other_id, doc_id, "temporal", 0.85,
                                          {"reason": "learning_progression", "auto_detected": True})
                        relationships_added += 1
                        print(f"      ‚úÖ Learning progression: {other_id} ‚Üí {doc_id}")
                        break
            
            # Strategy 4: Associative relationships (shared tags)
            if doc_tags and relationships_added < max_connections:
                for other_id in db.list_vectors():
                    if other_id == doc_id or relationships_added >= max_connections:
                        continue
                    
                    other_meta = db.get_vector(other_id)["metadata"]
                    other_tags = set(other_meta.get("tags", []))
                    shared_tags = doc_tags & other_tags
                    
                    if len(shared_tags) >= 2:  # Strong tag overlap
                        strength = min(0.7, len(shared_tags) * 0.2)
                        db.add_relationship(doc_id, other_id, "associative", strength,
                                          {"reason": "shared_tags", "tags": list(shared_tags), "auto_detected": True})
                        relationships_added += 1
                        print(f"      ‚úÖ Associative: {doc_id} ‚Üî {other_id} (tags: {shared_tags})")
                        break
            
            return relationships_added
        
        print("‚úÖ Strategic Relationship Building:")
        
        # Build relationships for existing vectors
        total_relationships = 0
        for vector_id in db.list_vectors():
            vector = db.get_vector(vector_id)
            if vector:
                print(f"   üîó Building relationships for {vector_id}:")
                relationships = build_smart_relationships(db, vector_id, vector["metadata"])
                total_relationships += relationships
        
        print(f"\nüìä Relationship Building Results:")
        print(f"   Total relationships created: {total_relationships}")
        print(f"   Database: {db.vector_count()} vectors, {db.relationship_count()} relationships")
        
        # ‚ùå BAD: Poor relationship strategies
        print(f"\n‚ùå What NOT to do - Poor Relationship Strategies:")
        bad_practices = [
            "Connecting everything to everything (creates noise)",
            "Using only one relationship type (limits discovery)",
            "Random relationship strengths (0.1, 0.9 without logic)",
            "No metadata-based relationship logic",
            "Creating circular dependencies without purpose",
            "Ignoring relationship capacity limits"
        ]
        
        for i, bad_practice in enumerate(bad_practices, 1):
            print(f"   {i}. ‚ùå {bad_practice}")
        
        print(f"\nüí° Relationship Building Best Practices:")
        relationship_practices = [
            "Start with hierarchical relationships (prerequisites)",
            "Build semantic relationships within categories",
            "Create temporal relationships for learning paths",
            "Use associative relationships for cross-references",
            "Reserve causal relationships for clear cause-effect",
            "Limit connections per vector (3-5 typically)",
            "Use meaningful relationship strengths (0.3-0.9)",
            "Add metadata to explain relationship reasoning",
            "Monitor capacity usage (500 relationships in Opin)",
            "Prioritize quality over quantity"
        ]
        
        for i, practice in enumerate(relationship_practices, 1):
            print(f"   {i}. ‚úÖ {practice}")
        
        return total_relationships
    
    def demonstrate_search_optimization(self, db):
        """Best practices for search optimization"""
        
        print("\nüîç Best Practice #3: Effective Search Strategies")
        print("-" * 45)
        
        # Create a sample query
        query_text = "machine learning algorithms for beginners"
        query_embedding = np.random.rand(384).astype(np.float32)  # Mock query embedding
        
        print(f"Query: '{query_text}'")
        
        # Strategy 1: Progressive search (start narrow, expand if needed)
        print("\n‚úÖ Strategy 1: Progressive Search")
        
        def progressive_search(db, query_embedding, target_results=5):
            """Progressive search: start precise, expand as needed"""
            
            strategies = [
                ("precise", {"top_k": target_results, "include_relationships": False, 
                           "similarity_threshold": 0.5}),
                ("balanced", {"top_k": target_results, "include_relationships": True, 
                            "max_hops": 1, "relationship_weight": 0.3}),
                ("broad", {"top_k": target_results, "include_relationships": True, 
                         "max_hops": 2, "relationship_weight": 0.5, "similarity_threshold": 0.1})
            ]
            
            for strategy_name, params in strategies:
                search_params = rudradb.SearchParams(**params)
                results = db.search(query_embedding, search_params)
                
                print(f"   {strategy_name.title()} search: {len(results)} results")
                
                if len(results) >= target_results:
                    print(f"   ‚úÖ Found sufficient results with {strategy_name} strategy")
                    return results, strategy_name
            
            return results, "broad"
        
        results, strategy_used = progressive_search(db, query_embedding)
        print(f"   Used strategy: {strategy_used}")
        
        # Strategy 2: Context-aware search
        print("\n‚úÖ Strategy 2: Context-Aware Search")
        
        def context_aware_search(db, query_embedding, context="general"):
            """Adjust search parameters based on context"""
            
            context_configs = {
                "learning": {  # For educational content
                    "top_k": 8,
                    "include_relationships": True,
                    "max_hops": 2,
                    "relationship_types": ["hierarchical", "temporal"],
                    "relationship_weight": 0.4
                },
                "research": {  # For research/discovery
                    "top_k": 12,
                    "include_relationships": True,
                    "max_hops": 2,
                    "relationship_weight": 0.6,
                    "similarity_threshold": 0.1
                },
                "precise": {  # For exact matches
                    "top_k": 5,
                    "include_relationships": False,
                    "similarity_threshold": 0.4
                },
                "general": {  # Balanced approach
                    "top_k": 8,
                    "include_relationships": True,
                    "max_hops": 2,
                    "relationship_weight": 0.3
                }
            }
            
            config = context_configs.get(context, context_configs["general"])
            return db.search(query_embedding, rudradb.SearchParams(**config))
        
        contexts = ["learning", "research", "precise", "general"]
        for context in contexts:
            results = context_aware_search(db, query_embedding, context)
            print(f"   {context.title()} context: {len(results)} results")
        
        # Strategy 3: Result analysis and filtering
        print("\n‚úÖ Strategy 3: Smart Result Analysis")
        
        def analyze_search_results(results, db):
            """Analyze and categorize search results"""
            
            analysis = {
                "direct_matches": [],
                "relationship_discoveries": [],
                "by_difficulty": {"beginner": 0, "intermediate": 0, "advanced": 0},
                "by_type": {},
                "avg_score": 0,
                "score_range": (1.0, 0.0)
            }
            
            scores = []
            for result in results:
                vector = db.get_vector(result.vector_id)
                if not vector:
                    continue
                
                metadata = vector["metadata"]
                
                # Categorize by connection type
                if result.hop_count == 0:
                    analysis["direct_matches"].append(result.vector_id)
                else:
                    analysis["relationship_discoveries"].append({
                        "id": result.vector_id,
                        "hops": result.hop_count
                    })
                
                # Categorize by difficulty
                difficulty = metadata.get("difficulty", "intermediate")
                analysis["by_difficulty"][difficulty] += 1
                
                # Categorize by type
                doc_type = metadata.get("type", "unknown")
                analysis["by_type"][doc_type] = analysis["by_type"].get(doc_type, 0) + 1
                
                # Score analysis
                score = result.combined_score
                scores.append(score)
                analysis["score_range"] = (
                    min(analysis["score_range"][0], score),
                    max(analysis["score_range"][1], score)
                )
            
            if scores:
                analysis["avg_score"] = sum(scores) / len(scores)
            
            return analysis
        
        # Analyze results from general context search
        general_results = context_aware_search(db, query_embedding, "general")
        analysis = analyze_search_results(general_results, db)
        
        print(f"   üìä Result Analysis:")
        print(f"      Direct matches: {len(analysis['direct_matches'])}")
        print(f"      Relationship discoveries: {len(analysis['relationship_discoveries'])}")
        print(f"      Average score: {analysis['avg_score']:.3f}")
        print(f"      Score range: {analysis['score_range'][0]:.3f} - {analysis['score_range'][1]:.3f}")
        print(f"      By difficulty: {analysis['by_difficulty']}")
        print(f"      By type: {analysis['by_type']}")
        
        # ‚ùå BAD: Poor search practices
        print(f"\n‚ùå What NOT to do - Poor Search Practices:")
        bad_search_practices = [
            "Always using maximum top_k (wastes resources)",
            "Ignoring similarity_threshold (returns irrelevant results)",
            "Using max_hops=2 always (can be slow)",
            "Not adjusting relationship_weight for context",
            "Ignoring result analysis and filtering",
            "Not using progressive search strategies"
        ]
        
        for i, bad_practice in enumerate(bad_search_practices, 1):
            print(f"   {i}. ‚ùå {bad_practice}")
        
        print(f"\nüí° Search Optimization Best Practices:")
        search_practices = [
            "Use progressive search (narrow ‚Üí broad)",
            "Adapt search parameters to context",
            "Set appropriate similarity_threshold (0.1-0.5)",
            "Balance relationship_weight (0.2-0.6 typical)",
            "Limit top_k to what you actually need",
            "Use max_hops strategically (1 for speed, 2 for discovery)",
            "Filter relationship_types when relevant",
            "Analyze results to improve future searches",
            "Cache frequent queries if possible",
            "Monitor search performance and adjust"
        ]
        
        for i, practice in enumerate(search_practices, 1):
            print(f"   {i}. ‚úÖ {practice}")
    
    def demonstrate_capacity_management(self, db):
        """Best practices for capacity management"""
        
        print("\nüìä Best Practice #4: Smart Capacity Management")
        print("-" * 45)
        
        # Get current capacity status
        stats = db.get_statistics()
        capacity = stats["capacity_usage"]
        
        print(f"üìà Current Capacity Status:")
        print(f"   Vectors: {stats['vector_count']}/{rudradb.MAX_VECTORS} ({capacity['vector_usage_percent']:.1f}%)")
        print(f"   Relationships: {stats['relationship_count']}/{rudradb.MAX_RELATIONSHIPS} ({capacity['relationship_usage_percent']:.1f}%)")
        
        # Strategy 1: Capacity monitoring
        def monitor_capacity(db, warn_threshold=80, critical_threshold=95):
            """Monitor capacity and provide guidance"""
            
            stats = db.get_statistics()
            capacity = stats["capacity_usage"]
            
            warnings = []
            recommendations = []
            
            # Vector capacity
            vector_pct = capacity["vector_usage_percent"]
            if vector_pct >= critical_threshold:
                warnings.append(f"Vector capacity critical ({vector_pct:.1f}%)")
                recommendations.append("Consider upgrading to full RudraDB")
            elif vector_pct >= warn_threshold:
                warnings.append(f"Vector capacity high ({vector_pct:.1f}%)")
                recommendations.append("Plan for upgrade or optimize current vectors")
            
            # Relationship capacity
            rel_pct = capacity["relationship_usage_percent"]
            if rel_pct >= critical_threshold:
                warnings.append(f"Relationship capacity critical ({rel_pct:.1f}%)")
                recommendations.append("Remove weak relationships or upgrade")
            elif rel_pct >= warn_threshold:
                warnings.append(f"Relationship capacity high ({rel_pct:.1f}%)")
                recommendations.append("Focus on high-quality relationships")
            
            return {
                "warnings": warnings,
                "recommendations": recommendations,
                "status": "critical" if any("critical" in w for w in warnings) else 
                         "warning" if warnings else "healthy"
            }
        
        capacity_status = monitor_capacity(db)
        
        print(f"\nüîç Capacity Analysis:")
        print(f"   Status: {capacity_status['status'].upper()}")
        
        if capacity_status['warnings']:
            for warning in capacity_status['warnings']:
                print(f"   ‚ö†Ô∏è {warning}")
        
        if capacity_status['recommendations']:
            for rec in capacity_status['recommendations']:
                print(f"   üí° {rec}")
        
        # Strategy 2: Quality over quantity
        print(f"\n‚úÖ Strategy: Quality Over Quantity")
        
        def analyze_data_quality(db):
            """Analyze data quality metrics"""
            
            # Relationship strength analysis
            strengths = []
            relationship_types = {}
            
            for vector_id in db.list_vectors():
                relationships = db.get_relationships(vector_id)
                for rel in relationships:
                    strengths.append(rel["strength"])
                    rel_type = rel["relationship_type"]
                    relationship_types[rel_type] = relationship_types.get(rel_type, 0) + 1
            
            # Metadata completeness
            metadata_completeness = []
            for vector_id in db.list_vectors():
                vector = db.get_vector(vector_id)
                if vector:
                    metadata = vector["metadata"]
                    # Count non-empty metadata fields
                    complete_fields = sum(1 for v in metadata.values() if v and v != "")
                    metadata_completeness.append(complete_fields)
            
            return {
                "avg_relationship_strength": sum(strengths) / len(strengths) if strengths else 0,
                "relationship_type_diversity": len(relationship_types),
                "relationship_distribution": relationship_types,
                "avg_metadata_fields": sum(metadata_completeness) / len(metadata_completeness) if metadata_completeness else 0,
                "weak_relationships": sum(1 for s in strengths if s < 0.3)
            }
        
        quality_metrics = analyze_data_quality(db)
        
        print(f"   üìä Data Quality Metrics:")
        print(f"      Average relationship strength: {quality_metrics['avg_relationship_strength']:.2f}")
        print(f"      Relationship type diversity: {quality_metrics['relationship_type_diversity']}/5")
        print(f"      Average metadata fields: {quality_metrics['avg_metadata_fields']:.1f}")
        print(f"      Weak relationships (<0.3): {quality_metrics['weak_relationships']}")
        
        # Strategy 3: Optimization suggestions
        print(f"\nüí° Capacity Management Best Practices:")
        capacity_practices = [
            "Monitor capacity usage regularly (aim for <90%)",
            "Focus on high-quality, meaningful relationships",
            "Remove relationships with strength < 0.3",
            "Use diverse relationship types for rich modeling",
            "Maintain complete, structured metadata",
            "Plan upgrade path when approaching 80% capacity",
            "Archive or remove test/experimental data",
            "Prioritize relationships that enhance search",
            "Use capacity limits as learning constraints",
            "Export data before major cleanup operations"
        ]
        
        for i, practice in enumerate(capacity_practices, 1):
            print(f"   {i}. ‚úÖ {practice}")
        
        return quality_metrics
    
    def demonstrate_performance_optimization(self, db):
        """Best practices for performance optimization"""
        
        print("\n‚ö° Best Practice #5: Performance Optimization")
        print("-" * 45)
        
        # Performance measurement
        def measure_operation_performance(db):
            """Measure performance of key operations"""
            
            results = {}
            
            # Vector addition performance
            start_time = time.time()
            test_vectors = 10
            
            for i in range(test_vectors):
                embedding = np.random.rand(384).astype(np.float32)
                metadata = {"test": True, "index": i}
                try:
                    db.add_vector(f"perf_test_{i}", embedding, metadata)
                except:
                    break  # May hit capacity
            
            vector_add_time = (time.time() - start_time) / test_vectors
            results["vector_add_ms"] = vector_add_time * 1000
            
            # Search performance
            query_embedding = np.random.rand(384).astype(np.float32)
            search_iterations = 20
            
            start_time = time.time()
            for _ in range(search_iterations):
                search_results = db.search(query_embedding, rudradb.SearchParams(
                    top_k=5,
                    include_relationships=True,
                    max_hops=2
                ))
            
            search_time = (time.time() - start_time) / search_iterations
            results["search_ms"] = search_time * 1000
            
            # Memory estimation
            stats = db.get_statistics()
            estimated_memory_mb = (
                stats["vector_count"] * stats["dimension"] * 4 +  # 4 bytes per float32
                stats["relationship_count"] * 200  # Estimate per relationship
            ) / (1024 * 1024)
            results["memory_mb"] = estimated_memory_mb
            
            return results
        
        perf_results = measure_operation_performance(db)
        
        print(f"üìä Performance Metrics:")
        print(f"   Vector addition: {perf_results['vector_add_ms']:.2f}ms per vector")
        print(f"   Search: {perf_results['search_ms']:.2f}ms per query")
        print(f"   Estimated memory: {perf_results['memory_mb']:.2f}MB")
        
        # Performance tips
        print(f"\nüí° Performance Best Practices:")
        perf_practices = [
            "Use np.float32 for embeddings (not float64)",
            "Keep metadata reasonably sized (<1KB per vector)",
            "Set appropriate similarity_threshold (0.1-0.5)",
            "Use top_k only for results you need",
            "Limit max_hops for faster relationship search",
            "Filter relationship_types when possible",
            "Batch operations when adding multiple vectors",
            "Monitor memory usage with large embeddings",
            "Consider dimension reduction for very high-D embeddings",
            "Profile search patterns to optimize parameters"
        ]
        
        for i, practice in enumerate(perf_practices, 1):
            print(f"   {i}. ‚úÖ {practice}")
        
        # Performance comparison
        print(f"\nüèÅ Performance Comparison (RudraDB-Opin vs Others):")
        comparisons = [
            "‚úÖ Opin: Optimized for 100 vectors (millisecond operations)",
            "‚úÖ Opin: Relationship-aware search built-in",
            "‚úÖ Opin: Auto-dimension detection (zero config)",  
            "‚úÖ Opin: Perfect for learning and prototyping",
            "üöÄ Full RudraDB: Optimized for 100K+ vectors",
            "üìà Traditional DBs: Only similarity search"
        ]
        
        for comparison in comparisons:
            print(f"   {comparison}")

def main():
    """Run comprehensive best practices demonstration"""
    
    demo = BestPracticesDemo()
    
    # Run all best practice demonstrations
    db = demo.demonstrate_metadata_design()
    relationships_created = demo.demonstrate_relationship_strategies(db)
    demo.demonstrate_search_optimization(db)
    quality_metrics = demo.demonstrate_capacity_management(db)
    demo.demonstrate_performance_optimization(db)
    
    # Final summary
    stats = db.get_statistics()
    
    print(f"\nüéâ Best Practices Demo Complete!")
    print("=" * 40)
    
    print(f"\nüìä Final Database State:")
    print(f"   Vectors: {stats['vector_count']}/{rudradb.MAX_VECTORS}")
    print(f"   Relationships: {stats['relationship_count']}/{rudradb.MAX_RELATIONSHIPS}")
    print(f"   Dimension: {stats['dimension']}D")
    print(f"   Relationships created: {relationships_created}")
    
    print(f"\nüèÜ Best Practices Mastered:")
    practices_summary = [
        "‚úÖ Structured, consistent metadata design",
        "‚úÖ Strategic relationship building (quality over quantity)",
        "‚úÖ Progressive and context-aware search strategies",
        "‚úÖ Smart capacity management and monitoring",
        "‚úÖ Performance optimization techniques"
    ]
    
    for practice in practices_summary:
        print(f"   {practice}")
    
    print(f"\nüöÄ Ready for Production:")
    readiness_indicators = [
        f"üìà High data quality (avg strength: {quality_metrics['avg_relationship_strength']:.2f})",
        f"üéØ Relationship diversity ({quality_metrics['relationship_type_diversity']}/5 types)",
        f"‚ö° Optimal performance patterns learned",
        f"üìä Capacity management strategies in place",
        f"üîç Advanced search techniques mastered"
    ]
    
    for indicator in readiness_indicators:
        print(f"   {indicator}")
    
    print(f"\nüí° Next Steps:")
    next_steps = [
        "Apply these patterns to your specific use case",
        "Build your production-ready application",
        "Monitor capacity and plan for upgrade",
        "Scale to full RudraDB when ready",
        "Share your learnings with the community!"
    ]
    
    for step in next_steps:
        print(f"   ‚Ä¢ {step}")

if __name__ == "__main__":
    main()
